# PyDataShred
This project emphasis on creating a wrapper for the modern data engineering which involves data shredding at its core

# Project Workflow:
------- ------
Python
    File Ingestion
    Database Ingestion - mysql
Pyspark
    File Ingestion
    Databse Ingestion
API
    Batch
    Streaming
    MetaData Ingestion Framework | Multi-Cloud Approach
SCD
    Spark / Non Spark

Frontend:
    FastHtml
    SvelteKit

Orchestration:
    Airflow / Apache Beam / Step Functions
    
Multi Cloud Support
    AWS/Azure/GCS - Databricks

Sentry.io Integration

Splunk Integration  -  Optional

Snowflake /SnowPark integration - using https://docs.snowflake.com/en/developer-guide/snowpark/python/testing-python-snowpark


# Multi Cloud Accounts
Google
AWS
Azure
Google Cloud
Snowflake
Heroku

<!-- Coding enviroment -->
Gitpod

# Random Articles
Steps to install mysql in codespace
https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-20-04
install mysql : sudo apt install mysql-server

<!-- >>> sudo su
     >>> sudo service mysql start | gitpod or codespace wont support systemctl so using service
     >>> mysql -u scott -p <-login -->

what is pydantic??
